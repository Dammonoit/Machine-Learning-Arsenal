{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Networks**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is neural netoworks?**\n",
    "\n",
    "**Neural Networks consist of the following components**\n",
    "\n",
    "* An input layer, x\n",
    "* An arbitrary amount of hidden layers\n",
    "* An output layer, ŷ\n",
    "* A set of weights and biases between each layer, W and b\n",
    "* A choice of activation function for each hidden layer, σ.\n",
    "\n",
    "The diagram below shows the architecture of a 2-layer Neural Network (note that the input layer is typically excluded when counting the number of layers in a Neural Network)\n",
    "![NN](https://i.imgur.com/x9apiYq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Full Batch Gradient Descent and Stochastic Gradient Descent.**\n",
    "* Both variants of Gradient Descent perform the same work of updating the weights of the MLP by using the same updating algorithm but the difference lies in the number of training samples used to update the weights and biases.\n",
    "\n",
    "* Full Batch Gradient Descent Algorithm as the name implies uses all the training data points to update each of the weights once whereas Stochastic Gradient uses 1 or more(sample) but never the entire training data to update the weights once.\n",
    "\n",
    "* Let us understand this with a simple example of a dataset of 10 data points with two weights w1 and w2.\n",
    "    * **Full Batch:** You use 10 data points (entire training data) and calculate the change in w1 (Δw1) and change in w2(Δw2) and update w1 and w2.\n",
    "\n",
    "    * **SGD:** You use 1st data point and calculate the change in w1 (Δw1) and change in w2(Δw2) and update w1 and w2. Next, when you use 2nd data point, you will work on the updated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are Activation functions and what are it uses in a Neural Network Model?**\n",
    "* Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.\n",
    "* They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n",
    "* Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n",
    "* Also another important feature of a Activation function is that it should be differentiable. \n",
    "* We need it to be this way so as to perform backpropogation optimization strategy while propogating backwards in the network to compute gradients of Error(loss) with respect to Weights and then accordingly optimize weights using Gradient descend or any other Optimization technique to reduce Error.\n",
    "\n",
    "### **The question arises that why can’t we do it without activating the input signal?**\n",
    "* If we do not apply a Activation function then the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree.\n",
    "* Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. \n",
    "* A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times. \n",
    "* We want our Neural Network to not just learn and compute a linear function but something more complicated than that. Also without activation function our Neural network would not be able to learn and model other complicated kinds of data such as images, videos , audio , speech etc. \n",
    "* That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear -big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets.\n",
    "\n",
    "### **So why do we need Non-Linearities?**\n",
    "* Non-linear functions are those which have degree more than one and they have a curvature when we plot a Non-Linear function. \n",
    "* Now we need a Neural Network Model to learn and represent almost anything and any arbitrary complex function which maps inputs to outputs. Neural-Networks are considered Universal Function Approximators. \n",
    "* It means that they can compute and learn any function at all. Almost any process we can think of can be represented as a functional computation in Neural Networks.\n",
    "\n",
    "* Hence it all comes down to this, we need to apply a Activation function f(x) so as to make the network more powerfull and add ability to it to learn something complex and complicated form data and represent non-linear complex arbitrary functional mappings between inputs and outputs. \n",
    "* Hence using a non linear Activation we are able to generate non-linear mappings from inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Most popular types of Activation functions :-**\n",
    "1. **Sigmoid or Logistic.**\n",
    "    * Sigmoid Activation function: It is a activation function of form f(x) = 1 / 1 + exp(-x) . \n",
    "    * Its Range is between 0 and 1. \n",
    "    * It is a S — shaped curve. \n",
    "    * It is easy to understand and apply but it has major reasons which have made it fall out of popularity such as :     \n",
    "      * Vanishing gradient problem\n",
    "      * Secondly , its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
    "    * Sigmoids saturate and kill gradients.\n",
    "    * Sigmoids have slow convergence.\n",
    "\n",
    "2. **Tanh — Hyperbolic tangent.**\n",
    "     * It’s mathamatical formula is f(x) = 1 — exp(-2x) / 1 + exp(-2x). \n",
    "     * Now it’s output is zero centered because its range in between -1 to 1 i.e -1 < output < 1 . \n",
    "     * Hence optimization is easier in this method hence in practice it is always preferred over Sigmoid function . But still it suffers from Vanishing gradient problem. \n",
    "3. **ReLu -Rectified linear units.**\n",
    "   * It has become very popular in the past couple of years. \n",
    "   * It was recently proved that it had 6 times improvement in convergence from Tanh function. \n",
    "   * It’s just R(x) = max(0,x) i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x. Hence as seeing the mathamatical form of this function we can see that it is very simple and efficinent . \n",
    "   * A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. \n",
    "   * Hence it avoids and rectifies vanishing gradient problem . \n",
    "   * Almost all deep learning Models use ReLu nowadays.\n",
    "   * Another problem with ReLu is that some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n",
    "       * To fix this problem another modification was introduced called Leaky ReLu to fix the problem of dying neurons. It introduces a small slope to keep the updates alive.\n",
    "       * We then have another variant made form both ReLu and Leaky ReLu called Maxout function . \n",
    "       * But its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First off, what is a learning rate?**\n",
    "* Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. \n",
    "* The lower the value, the slower we travel along the downward slope. \n",
    "* While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region.\n",
    "* The following formula shows the relationship.\n",
    "    * new_weight = existing_weight — learning_rate * gradient\n",
    "\n",
    "* Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.\n",
    "\n",
    "* As such, it’s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate.\n",
    "* Furthermore, the learning rate affects how quickly our model can converge to a local minima (aka arrive at the best accuracy). Thus getting it right from the get go would mean lesser time for us to train the model.\n",
    "* **Less training time, lesser money spent on GPU cloud compute. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
